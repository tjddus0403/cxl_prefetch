{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library & Module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sykim/miniconda3/envs/prefetch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_83556/533703381.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    perc_train = 0.5,\n",
    "    perc_val = 0.2,\n",
    "    perc_test = 0.3,\n",
    "    perc_vocab = 1,\n",
    "    dataset = \"../deep_learning_data/pr_p4.cstate\",\n",
    "    dataset_csv = \"pr_p_4m.csv\",\n",
    "    # dataset_csv = \"bert_pf_before.csv\",\n",
    "    seed = 1337,\n",
    "    lr = 5e-4,\n",
    "    batch_size = 32,\n",
    "    num_epoch = 200,\n",
    "    embedding_size = 64,\n",
    "    encoding_size = 32,\n",
    "    cut_off = 1,\n",
    "    max_len = 128,\n",
    "    cuda = True,\n",
    "    device = 'cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# 재현성을 위해 시드 설정\n",
    "set_seed_everywhere(args.seed, args.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(args.dataset_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pa</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43228639245 37684496844 41786200568 1199737384...</td>\n",
       "      <td>8330617572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8330617572 43228639245 37684496844 41786200568...</td>\n",
       "      <td>49004211181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49004211181 8330617572 43228639245 37684496844...</td>\n",
       "      <td>60166466637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60166466637 49004211181 8330617572 43228639245...</td>\n",
       "      <td>52769089427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52769089427 60166466637 49004211181 8330617572...</td>\n",
       "      <td>20709542610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999995</th>\n",
       "      <td>26689522992 66675701356 36960972436 8204552737...</td>\n",
       "      <td>34331113062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999996</th>\n",
       "      <td>34331113062 26689522992 66675701356 3696097243...</td>\n",
       "      <td>65558066556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999997</th>\n",
       "      <td>65558066556 34331113062 26689522992 6667570135...</td>\n",
       "      <td>17661771520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999998</th>\n",
       "      <td>17661771520 65558066556 34331113062 2668952299...</td>\n",
       "      <td>29786412262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999999</th>\n",
       "      <td>29786412262 17661771520 65558066556 3433111306...</td>\n",
       "      <td>57138307638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        pa        label\n",
       "0        43228639245 37684496844 41786200568 1199737384...   8330617572\n",
       "1        8330617572 43228639245 37684496844 41786200568...  49004211181\n",
       "2        49004211181 8330617572 43228639245 37684496844...  60166466637\n",
       "3        60166466637 49004211181 8330617572 43228639245...  52769089427\n",
       "4        52769089427 60166466637 49004211181 8330617572...  20709542610\n",
       "...                                                    ...          ...\n",
       "3999995  26689522992 66675701356 36960972436 8204552737...  34331113062\n",
       "3999996  34331113062 26689522992 66675701356 3696097243...  65558066556\n",
       "3999997  65558066556 34331113062 26689522992 6667570135...  17661771520\n",
       "3999998  17661771520 65558066556 34331113062 2668952299...  29786412262\n",
       "3999999  29786412262 17661771520 65558066556 3433111306...  57138307638\n",
       "\n",
       "[4000000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['pa'] = dataset_df['pa'].shift(periods=2, axis=0)\n",
    "dataset_df = dataset_df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pa</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43228639245 37684496844 41786200568 1199737384...</td>\n",
       "      <td>60166466637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8330617572 43228639245 37684496844 41786200568...</td>\n",
       "      <td>52769089427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49004211181 8330617572 43228639245 37684496844...</td>\n",
       "      <td>20709542610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60166466637 49004211181 8330617572 43228639245...</td>\n",
       "      <td>2114328522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>52769089427 60166466637 49004211181 8330617572...</td>\n",
       "      <td>15300928092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>14775837574 60205375080 34901346494 4942601769...</td>\n",
       "      <td>31376783447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>450494791 14775837574 60205375080 34901346494 ...</td>\n",
       "      <td>17266583504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>41439924855 450494791 14775837574 60205375080 ...</td>\n",
       "      <td>10362117563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000000</th>\n",
       "      <td>68115065202 31376783447 41439924855 450494791 ...</td>\n",
       "      <td>24571272764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000001</th>\n",
       "      <td>17266583504 68115065202 31376783447 4143992485...</td>\n",
       "      <td>23112974339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        pa        label\n",
       "2        43228639245 37684496844 41786200568 1199737384...  60166466637\n",
       "3        8330617572 43228639245 37684496844 41786200568...  52769089427\n",
       "4        49004211181 8330617572 43228639245 37684496844...  20709542610\n",
       "5        60166466637 49004211181 8330617572 43228639245...   2114328522\n",
       "6        52769089427 60166466637 49004211181 8330617572...  15300928092\n",
       "...                                                    ...          ...\n",
       "999997   14775837574 60205375080 34901346494 4942601769...  31376783447\n",
       "999998   450494791 14775837574 60205375080 34901346494 ...  17266583504\n",
       "999999   41439924855 450494791 14775837574 60205375080 ...  10362117563\n",
       "1000000  68115065202 31376783447 41439924855 450494791 ...  24571272764\n",
       "1000001  17266583504 68115065202 31376783447 4143992485...  23112974339\n",
       "\n",
       "[1000000 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = dataset_df.iloc[:1000000]\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['label'] = dataset_df['label'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pa</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43228639245 37684496844 41786200568 1199737384...</td>\n",
       "      <td>60166466637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8330617572 43228639245 37684496844 41786200568...</td>\n",
       "      <td>52769089427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49004211181 8330617572 43228639245 37684496844...</td>\n",
       "      <td>20709542610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60166466637 49004211181 8330617572 43228639245...</td>\n",
       "      <td>2114328522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52769089427 60166466637 49004211181 8330617572...</td>\n",
       "      <td>15300928092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>14775837574 60205375080 34901346494 4942601769...</td>\n",
       "      <td>31376783447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>450494791 14775837574 60205375080 34901346494 ...</td>\n",
       "      <td>17266583504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>41439924855 450494791 14775837574 60205375080 ...</td>\n",
       "      <td>10362117563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>68115065202 31376783447 41439924855 450494791 ...</td>\n",
       "      <td>24571272764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>17266583504 68115065202 31376783447 4143992485...</td>\n",
       "      <td>23112974339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       pa        label\n",
       "0       43228639245 37684496844 41786200568 1199737384...  60166466637\n",
       "1       8330617572 43228639245 37684496844 41786200568...  52769089427\n",
       "2       49004211181 8330617572 43228639245 37684496844...  20709542610\n",
       "3       60166466637 49004211181 8330617572 43228639245...   2114328522\n",
       "4       52769089427 60166466637 49004211181 8330617572...  15300928092\n",
       "...                                                   ...          ...\n",
       "999995  14775837574 60205375080 34901346494 4942601769...  31376783447\n",
       "999996  450494791 14775837574 60205375080 34901346494 ...  17266583504\n",
       "999997  41439924855 450494791 14775837574 60205375080 ...  10362117563\n",
       "999998  68115065202 31376783447 41439924855 450494791 ...  24571272764\n",
       "999999  17266583504 68115065202 31376783447 4143992485...  23112974339\n",
       "\n",
       "[1000000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = dataset_df.reset_index(drop=True)\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datatset / DataLoader / Vectorizer / Vocabulary / Model의 흐름에 대한 구성\n",
    "순서 : Vocabulary -> Vectorizer -> Dataset -> DataLoader -> Model\n",
    "\n",
    "Raw Data\n",
    "- 현 시점 cache에 존재하는 주소 목록 \n",
    "- ex: 125, 158, 154, 134, 145, 341, 133, 136\n",
    "- 주소를 숫자가 아닌 문자열로 보아야 함 (physical addr는 숫자의 의미보다 문자열의 의미가 더 강하다고 판단)\n",
    "- 주소를 하나의 토큰으로\n",
    "\n",
    "Vocabulary \n",
    "- 각 주소(토큰)를 정수로 매핑\n",
    "- 입력 및 출력의 대상이 되는 주소만을 가짐 (+-10정도 커버?)\n",
    "\n",
    "** Vectorizer ** \n",
    "- 매핑된 토큰을 벡터 형태로 변환\n",
    "\n",
    "Dataset \n",
    "- vectorizer 이용하여 구성\n",
    "\n",
    "DataLoader\n",
    "- 미니배치 단위로 데이터셋 가져옴\n",
    "\n",
    "Model\n",
    "- 초기값: 첫 주소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAVocabulary(object):\n",
    "    def __init__(self, token_to_idx = None, add_unk = True,\n",
    "                 mask_token = \"<MASK>\", unk_token = \"<UNK>\"):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx:token for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(self._unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, cstates):\n",
    "        return cls(**cstates)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" %index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\"%len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer\n",
    "- max_len에 대한 조건 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAVectorizer(object):\n",
    "    def __init__(self, pa_vocab):\n",
    "        self.pa_vocab = pa_vocab\n",
    "        # self.max_la_length = max_pa_length\n",
    "    \n",
    "    def _vectorize(self, indices):\n",
    "        vector_length = len(indices)\n",
    "        vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        vector[:len(indices)] = indices\n",
    "        return vector\n",
    "        \n",
    "    def _get_pa_indices(self, pa_list):\n",
    "        # print(pa_list)\n",
    "        # print(type(pa_list))\n",
    "        # 벡터로 변환된 physical addr list 반환\n",
    "        indices = [self.pa_vocab.lookup_token(token) for token in pa_list.split(\" \")]\n",
    "        return indices\n",
    "    \n",
    "    def vectorize(self, cstate):\n",
    "        pa_indices = self._get_pa_indices(cstate)\n",
    "        pa_vector = self._vectorize(indices=pa_indices)\n",
    "        return {'pa_vector':pa_vector,\n",
    "                'pa_length':len(pa_indices)}\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, cstate_df):\n",
    "        pa_vocab = PAVocabulary()\n",
    "        pa_counts = Counter()\n",
    "        for cstate in cstate_df.pa:\n",
    "            for pa in cstate.split(\" \"):\n",
    "                pa_counts[pa] += 1\n",
    "        for cstate in cstate_df.label:\n",
    "            pa_counts[cstate] += 1\n",
    "        \n",
    "        for pa, count in pa_counts.items():\n",
    "            if count >= args.cut_off:\n",
    "                pa_vocab.add_token(pa)\n",
    "        print(\"vectorizer vocab len: \",len(pa_vocab))\n",
    "        return cls(pa_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, cstate_dict):\n",
    "        pa_vocab = PAVocabulary.from_serializable(cstate_dict['pa_vocab'])\n",
    "        return cls(pa_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'pa_vocab': self.pa_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CstateDataset(Dataset):\n",
    "    def __init__(self, cstate_df, vectorizer):\n",
    "        self.cstate_df = cstate_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.n_total = len(cstate_df)\n",
    "        \n",
    "        self.train_size = int(self.n_total * args.perc_train)\n",
    "        self.train_df = self.cstate_df.loc[:self.train_size]\n",
    "        \n",
    "        self.val_size = int(self.n_total * args.perc_val)\n",
    "        self.val_df = self.cstate_df.loc[self.train_size : self.train_size+self.val_size]\n",
    "        \n",
    "        self.test_size = self.n_total - (self.train_size + self.val_size)\n",
    "        self.test_df = self.cstate_df.loc[self.train_size+self.val_size:]\n",
    "        \n",
    "        self.vocab_df_size = self.n_total\n",
    "        self.vocab_df = self.cstate_df\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.val_size),\n",
    "                             'test': (self.test_df, self.test_size),\n",
    "                             'vocab': (self.vocab_df, self.vocab_df_size)}\n",
    "        \n",
    "        self.set_split('vocab')\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, cstate_df):\n",
    "        # cstate_df = pd.read_csv(cstate_csv)\n",
    "        vocab_cstate_df = cstate_df.loc[:int(len(cstate_df)*args.perc_vocab)]\n",
    "        print(\"dataset df len:\", len(vocab_cstate_df))\n",
    "        vectorizer = PAVectorizer.from_dataframe(vocab_cstate_df)\n",
    "        \n",
    "        return cls(cstate_df, vectorizer)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, cstate_df, vectorizer_filepath):\n",
    "        vocab_cstate_df = cstate_df\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(vocab_cstate_df, vectorizer)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return PAVectorizer.from_serializable(json.load(fp))\n",
    "        \n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "            \n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split='train'):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        pa_vector = self._vectorizer.vectorize(row.pa)\n",
    "        label_vector = self._vectorizer.vectorize(row.label)\n",
    "        \n",
    "        return {'x_data': pa_vector['pa_vector'],\n",
    "                'y_target': label_vector['pa_vector'],\n",
    "                'x_data_length': pa_vector['pa_length']}\n",
    "        \n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=False, drop_last=True, device='cpu'):\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict={}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn_fc = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.attn_fc(x)\n",
    "        # print(x.shape)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prefetcher(nn.Module):\n",
    "    def __init__(self, args, vocab_size):\n",
    "        super(Prefetcher, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                      embedding_dim=args.embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size = args.embedding_size,\n",
    "                            hidden_size=args.encoding_size,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True,\n",
    "                            dropout=0.1)\n",
    "        self.attention = Attention(args.encoding_size*2)\n",
    "        self.fc2 = nn.Linear(in_features=args.max_len, out_features=vocab_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.embedding(x)\n",
    "        # print(x.shape)\n",
    "        output, hidden = self.lstm(x)\n",
    "        # print(output.shape)\n",
    "        attn_weights = self.attention(output)\n",
    "        # print(attn_weights.shape)\n",
    "        attn_output = output * attn_weights\n",
    "        # print(attn_output.shape)\n",
    "        attn_output = torch.sum(attn_output, dim=-1)\n",
    "        # print(attn_output.shape)\n",
    "        x = self.fc2(attn_output)\n",
    "        # print(x.shape)\n",
    "        x = self.softmax(x)\n",
    "        # print(x.shape)\n",
    "        x = x.squeeze()\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "dataset df len: 1000000\n",
      "vectorizer vocab len:  1600\n"
     ]
    }
   ],
   "source": [
    "# 현재 사용가능한 디바이스로 환경변수 device 재설정\n",
    "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(args.device)\n",
    "# 학습에 사용할 데이터셋 파일 가져와서 Dataset 객체 만들기 \n",
    "dataset = CstateDataset.load_dataset_and_make_vectorizer(dataset_df)\n",
    "# dataset 객체를 만들면 안에서 vectorizer 객체도 생성되기 때문에 여기서 vectorizer 뽑아낼 수 있음\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Prefetcher(args, len(vectorizer.pa_vocab)).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.pa_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test (X / X Steps): 0it [00:00, ?it/s]/tmp/ipykernel_83556/2672553311.py:30: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n",
      "Test (5 / 300000 Steps): : 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test (299999 / 300000 Steps): : 300000it [1:25:25, 58.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189068\n",
      "63.02266666666667\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/model_20.pth'))\n",
    "model = model.to(args.device)\n",
    "result_df = pd.DataFrame()\n",
    "dataset.set_split('test')\n",
    "test_batch_generator = generate_batches(dataset, batch_size=1, device=args.device)\n",
    "test_iterator = tqdm(test_batch_generator, desc=\"Test (X / X Steps)\", dynamic_ncols=True)\n",
    "test_total = dataset.get_num_batches(1)\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    # for _, data in enumerate(test_batch_generator):\n",
    "    for idx, data in enumerate(test_iterator):\n",
    "        output = model(data['x_data'].to(args.device))\n",
    "        topk_vals, topk_indices = torch.topk(output, 32)\n",
    "        toplist = topk_indices.detach().cpu().tolist()\n",
    "        target = data['y_target'].squeeze().cpu().detach().numpy()\n",
    "        output_df = pd.DataFrame({'pred':[toplist], 'target':target})\n",
    "        result_df = pd.concat([result_df, output_df])\n",
    "        if target in toplist:\n",
    "            correct+=1\n",
    "        test_iterator.set_description(\"Test (%d / %d Steps)\" %(idx, test_total))\n",
    "print(correct)\n",
    "print(correct/test_total*100)\n",
    "result_df.to_csv('test_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.52066666666666"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_10 = 0\n",
    "for idx, row in result_df.iterrows():\n",
    "    if row.target in row.pred[:10]:\n",
    "        cor_10+=1\n",
    "cor_10 / len(result_df) * 100\n",
    "# result_df.to_csv('test_result10.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.2905, -3.5925, -3.7852, -3.7986, -3.8506, -3.8742, -4.0041, -4.0068,\n",
       "        -4.0105, -4.0223])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_vals.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6231, 6180, 6182, 6444, 6103, 5933, 6216, 6226, 6262, 6081],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prefetch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
