{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library & Module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sykim/miniconda3/envs/prefetch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_90061/533703381.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    perc_train = 0.5,\n",
    "    perc_val = 0.2,\n",
    "    perc_test = 0.3,\n",
    "    perc_vocab = 1,\n",
    "    dataset = \"../deep_learning_data/pr_1MB.cstate\",\n",
    "    dataset_csv = \"pr_1MB.csv\",\n",
    "    # dataset_csv = \"bert_pf_before.csv\",\n",
    "    seed = 1337,\n",
    "    lr = 5e-4,\n",
    "    batch_size = 64,\n",
    "    num_epoch = 200,\n",
    "    embedding_size = 64,\n",
    "    encoding_size = 32,\n",
    "    cut_off = 1,\n",
    "    max_len = 128,\n",
    "    cuda = True,\n",
    "    device = 'cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# 재현성을 위해 시드 설정\n",
    "set_seed_everywhere(args.seed, args.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = list()\n",
    "output_list = list()\n",
    "with open(args.dataset, 'r') as dataset:\n",
    "    line = dataset.readline().split()\n",
    "    while line:\n",
    "        strline = [str(dstr) for dstr in line]\n",
    "        input_list.append(\" \".join(strline[:-1]))\n",
    "        output_list.append(strline[-1])\n",
    "        line = dataset.readline().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pa</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31854774143 31854774142 31854774141 3185477414...</td>\n",
       "      <td>42318532544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42318532591 42318532590 42318532589 4231853258...</td>\n",
       "      <td>48286355124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48286355135 48286355134 48286355133 4828635513...</td>\n",
       "      <td>23732197760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23732197823 23732197822 23732197821 2373219782...</td>\n",
       "      <td>7432544128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7432544179 7432544180 7432544178 7432544177 74...</td>\n",
       "      <td>65249514352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036778</th>\n",
       "      <td>23732197811 23732197809 23732197806 2373219780...</td>\n",
       "      <td>23732197819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036779</th>\n",
       "      <td>23732197819 23732197811 23732197809 2373219780...</td>\n",
       "      <td>23732197821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036780</th>\n",
       "      <td>23732197821 23732197819 23732197811 2373219780...</td>\n",
       "      <td>7432544128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036781</th>\n",
       "      <td>7432544130 7432544129 7432544128 23732197821 2...</td>\n",
       "      <td>7432544135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036782</th>\n",
       "      <td>7432544135 7432544130 7432544129 7432544128 23...</td>\n",
       "      <td>7432544139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4036783 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        pa        label\n",
       "0        31854774143 31854774142 31854774141 3185477414...  42318532544\n",
       "1        42318532591 42318532590 42318532589 4231853258...  48286355124\n",
       "2        48286355135 48286355134 48286355133 4828635513...  23732197760\n",
       "3        23732197823 23732197822 23732197821 2373219782...   7432544128\n",
       "4        7432544179 7432544180 7432544178 7432544177 74...  65249514352\n",
       "...                                                    ...          ...\n",
       "4036778  23732197811 23732197809 23732197806 2373219780...  23732197819\n",
       "4036779  23732197819 23732197811 23732197809 2373219780...  23732197821\n",
       "4036780  23732197821 23732197819 23732197811 2373219780...   7432544128\n",
       "4036781  7432544130 7432544129 7432544128 23732197821 2...   7432544135\n",
       "4036782  7432544135 7432544130 7432544129 7432544128 23...   7432544139\n",
       "\n",
       "[4036783 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = pd.DataFrame(input_list, columns=['pa'])\n",
    "final_data['label'] = output_list\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv(args.dataset_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(args.dataset_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pa</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31854774143 31854774142 31854774141 3185477414...</td>\n",
       "      <td>42318532544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42318532591 42318532590 42318532589 4231853258...</td>\n",
       "      <td>48286355124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48286355135 48286355134 48286355133 4828635513...</td>\n",
       "      <td>23732197760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23732197823 23732197822 23732197821 2373219782...</td>\n",
       "      <td>7432544128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7432544179 7432544180 7432544178 7432544177 74...</td>\n",
       "      <td>65249514352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036778</th>\n",
       "      <td>23732197811 23732197809 23732197806 2373219780...</td>\n",
       "      <td>23732197819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036779</th>\n",
       "      <td>23732197819 23732197811 23732197809 2373219780...</td>\n",
       "      <td>23732197821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036780</th>\n",
       "      <td>23732197821 23732197819 23732197811 2373219780...</td>\n",
       "      <td>7432544128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036781</th>\n",
       "      <td>7432544130 7432544129 7432544128 23732197821 2...</td>\n",
       "      <td>7432544135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036782</th>\n",
       "      <td>7432544135 7432544130 7432544129 7432544128 23...</td>\n",
       "      <td>7432544139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4036783 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        pa        label\n",
       "0        31854774143 31854774142 31854774141 3185477414...  42318532544\n",
       "1        42318532591 42318532590 42318532589 4231853258...  48286355124\n",
       "2        48286355135 48286355134 48286355133 4828635513...  23732197760\n",
       "3        23732197823 23732197822 23732197821 2373219782...   7432544128\n",
       "4        7432544179 7432544180 7432544178 7432544177 74...  65249514352\n",
       "...                                                    ...          ...\n",
       "4036778  23732197811 23732197809 23732197806 2373219780...  23732197819\n",
       "4036779  23732197819 23732197811 23732197809 2373219780...  23732197821\n",
       "4036780  23732197821 23732197819 23732197811 2373219780...   7432544128\n",
       "4036781  7432544130 7432544129 7432544128 23732197821 2...   7432544135\n",
       "4036782  7432544135 7432544130 7432544129 7432544128 23...   7432544139\n",
       "\n",
       "[4036783 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['pa'] = dataset_df['pa'].shift(periods=2, axis=0)\n",
    "dataset_df = dataset_df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pa</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31854774143 31854774142 31854774141 3185477414...</td>\n",
       "      <td>23732197760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42318532591 42318532590 42318532589 4231853258...</td>\n",
       "      <td>7432544128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48286355135 48286355134 48286355133 4828635513...</td>\n",
       "      <td>65249514352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23732197823 23732197822 23732197821 2373219782...</td>\n",
       "      <td>58858693312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7432544179 7432544180 7432544178 7432544177 74...</td>\n",
       "      <td>5016497408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>5016497451 5016497441 5016497433 5016497432 50...</td>\n",
       "      <td>40010374578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>40010374563 5016497451 5016497441 5016497433 5...</td>\n",
       "      <td>17690378394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>40010374570 40010374563 5016497451 5016497441 ...</td>\n",
       "      <td>17690378407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600000</th>\n",
       "      <td>40010374578 40010374570 40010374563 5016497451...</td>\n",
       "      <td>17690378423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600001</th>\n",
       "      <td>17690378395 17690378394 40010374578 4001037457...</td>\n",
       "      <td>29295405504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        pa        label\n",
       "2        31854774143 31854774142 31854774141 3185477414...  23732197760\n",
       "3        42318532591 42318532590 42318532589 4231853258...   7432544128\n",
       "4        48286355135 48286355134 48286355133 4828635513...  65249514352\n",
       "5        23732197823 23732197822 23732197821 2373219782...  58858693312\n",
       "6        7432544179 7432544180 7432544178 7432544177 74...   5016497408\n",
       "...                                                    ...          ...\n",
       "1599997  5016497451 5016497441 5016497433 5016497432 50...  40010374578\n",
       "1599998  40010374563 5016497451 5016497441 5016497433 5...  17690378394\n",
       "1599999  40010374570 40010374563 5016497451 5016497441 ...  17690378407\n",
       "1600000  40010374578 40010374570 40010374563 5016497451...  17690378423\n",
       "1600001  17690378395 17690378394 40010374578 4001037457...  29295405504\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = dataset_df.iloc[:1600000]\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['label'] = dataset_df['label'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pa</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31854774143 31854774142 31854774141 3185477414...</td>\n",
       "      <td>23732197760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42318532591 42318532590 42318532589 4231853258...</td>\n",
       "      <td>7432544128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48286355135 48286355134 48286355133 4828635513...</td>\n",
       "      <td>65249514352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23732197823 23732197822 23732197821 2373219782...</td>\n",
       "      <td>58858693312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7432544179 7432544180 7432544178 7432544177 74...</td>\n",
       "      <td>5016497408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>5016497451 5016497441 5016497433 5016497432 50...</td>\n",
       "      <td>40010374578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>40010374563 5016497451 5016497441 5016497433 5...</td>\n",
       "      <td>17690378394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>40010374570 40010374563 5016497451 5016497441 ...</td>\n",
       "      <td>17690378407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>40010374578 40010374570 40010374563 5016497451...</td>\n",
       "      <td>17690378423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>17690378395 17690378394 40010374578 4001037457...</td>\n",
       "      <td>29295405504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        pa        label\n",
       "0        31854774143 31854774142 31854774141 3185477414...  23732197760\n",
       "1        42318532591 42318532590 42318532589 4231853258...   7432544128\n",
       "2        48286355135 48286355134 48286355133 4828635513...  65249514352\n",
       "3        23732197823 23732197822 23732197821 2373219782...  58858693312\n",
       "4        7432544179 7432544180 7432544178 7432544177 74...   5016497408\n",
       "...                                                    ...          ...\n",
       "1599995  5016497451 5016497441 5016497433 5016497432 50...  40010374578\n",
       "1599996  40010374563 5016497451 5016497441 5016497433 5...  17690378394\n",
       "1599997  40010374570 40010374563 5016497451 5016497441 ...  17690378407\n",
       "1599998  40010374578 40010374570 40010374563 5016497451...  17690378423\n",
       "1599999  17690378395 17690378394 40010374578 4001037457...  29295405504\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = dataset_df.reset_index(drop=True)\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datatset / DataLoader / Vectorizer / Vocabulary / Model의 흐름에 대한 구성\n",
    "순서 : Vocabulary -> Vectorizer -> Dataset -> DataLoader -> Model\n",
    "\n",
    "Raw Data\n",
    "- 현 시점 cache에 존재하는 주소 목록 \n",
    "- ex: 125, 158, 154, 134, 145, 341, 133, 136\n",
    "- 주소를 숫자가 아닌 문자열로 보아야 함 (physical addr는 숫자의 의미보다 문자열의 의미가 더 강하다고 판단)\n",
    "- 주소를 하나의 토큰으로\n",
    "\n",
    "Vocabulary \n",
    "- 각 주소(토큰)를 정수로 매핑\n",
    "- 입력 및 출력의 대상이 되는 주소만을 가짐 (+-10정도 커버?)\n",
    "\n",
    "** Vectorizer ** \n",
    "- 매핑된 토큰을 벡터 형태로 변환\n",
    "\n",
    "Dataset \n",
    "- vectorizer 이용하여 구성\n",
    "\n",
    "DataLoader\n",
    "- 미니배치 단위로 데이터셋 가져옴\n",
    "\n",
    "Model\n",
    "- 초기값: 첫 주소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAVocabulary(object):\n",
    "    def __init__(self, token_to_idx = None, add_unk = True,\n",
    "                 mask_token = \"<MASK>\", unk_token = \"<UNK>\"):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx:token for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(self._unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, cstates):\n",
    "        return cls(**cstates)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" %index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\"%len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer\n",
    "- max_len에 대한 조건 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAVectorizer(object):\n",
    "    def __init__(self, pa_vocab):\n",
    "        self.pa_vocab = pa_vocab\n",
    "        # self.max_la_length = max_pa_length\n",
    "    \n",
    "    def _vectorize(self, indices):\n",
    "        vector_length = len(indices)\n",
    "        vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        vector[:len(indices)] = indices\n",
    "        return vector\n",
    "        \n",
    "    def _get_pa_indices(self, pa_list):\n",
    "        # print(pa_list)\n",
    "        # print(type(pa_list))\n",
    "        # 벡터로 변환된 physical addr list 반환\n",
    "        indices = [self.pa_vocab.lookup_token(token) for token in pa_list.split(\" \")]\n",
    "        return indices\n",
    "    \n",
    "    def vectorize(self, cstate):\n",
    "        pa_indices = self._get_pa_indices(cstate)\n",
    "        pa_vector = self._vectorize(indices=pa_indices)\n",
    "        return {'pa_vector':pa_vector,\n",
    "                'pa_length':len(pa_indices)}\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, cstate_df):\n",
    "        pa_vocab = PAVocabulary()\n",
    "        pa_counts = Counter()\n",
    "        for cstate in cstate_df.pa:\n",
    "            for pa in cstate.split(\" \"):\n",
    "                pa_counts[pa] += 1\n",
    "        for cstate in cstate_df.label:\n",
    "            pa_counts[cstate] += 1\n",
    "        \n",
    "        for pa, count in pa_counts.items():\n",
    "            if count >= args.cut_off:\n",
    "                pa_vocab.add_token(pa)\n",
    "        print(\"vectorizer vocab len: \",len(pa_vocab))\n",
    "        return cls(pa_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, cstate_dict):\n",
    "        pa_vocab = PAVocabulary.from_serializable(cstate_dict['pa_vocab'])\n",
    "        return cls(pa_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'pa_vocab': self.pa_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CstateDataset(Dataset):\n",
    "    def __init__(self, cstate_df, vectorizer):\n",
    "        self.cstate_df = cstate_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.n_total = len(cstate_df)\n",
    "        \n",
    "        self.train_size = int(self.n_total * args.perc_train)\n",
    "        self.train_df = self.cstate_df.loc[:self.train_size]\n",
    "        \n",
    "        self.val_size = int(self.n_total * args.perc_val)\n",
    "        self.val_df = self.cstate_df.loc[self.train_size : self.train_size+self.val_size]\n",
    "        \n",
    "        self.test_size = self.n_total - (self.train_size + self.val_size)\n",
    "        self.test_df = self.cstate_df.loc[self.train_size+self.val_size:]\n",
    "        \n",
    "        self.vocab_df_size = self.n_total\n",
    "        self.vocab_df = self.cstate_df\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.val_size),\n",
    "                             'test': (self.test_df, self.test_size),\n",
    "                             'vocab': (self.vocab_df, self.vocab_df_size)}\n",
    "        \n",
    "        self.set_split('vocab')\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, cstate_df):\n",
    "        # cstate_df = pd.read_csv(cstate_csv)\n",
    "        vocab_cstate_df = cstate_df.loc[:int(len(cstate_df)*args.perc_vocab)]\n",
    "        print(\"dataset df len:\", len(vocab_cstate_df))\n",
    "        vectorizer = PAVectorizer.from_dataframe(vocab_cstate_df)\n",
    "        \n",
    "        return cls(cstate_df, vectorizer)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, cstate_df, vectorizer_filepath):\n",
    "        vocab_cstate_df = cstate_df\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(vocab_cstate_df, vectorizer)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return PAVectorizer.from_serializable(json.load(fp))\n",
    "        \n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "            \n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split='train'):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        pa_vector = self._vectorizer.vectorize(row.pa)\n",
    "        label_vector = self._vectorizer.vectorize(row.label)\n",
    "        \n",
    "        return {'x_data': pa_vector['pa_vector'],\n",
    "                'y_target': label_vector['pa_vector'],\n",
    "                'x_data_length': pa_vector['pa_length']}\n",
    "        \n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=False, drop_last=True, device='cpu'):\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict={}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn_fc = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.attn_fc(x)\n",
    "        # print(x.shape)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prefetcher(nn.Module):\n",
    "    def __init__(self, args, vocab_size):\n",
    "        super(Prefetcher, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                      embedding_dim=args.embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size = args.embedding_size,\n",
    "                            hidden_size=args.encoding_size,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True,\n",
    "                            dropout=0.1)\n",
    "        self.attention = Attention(args.encoding_size*2)\n",
    "        self.fc2 = nn.Linear(in_features=args.max_len, out_features=vocab_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.embedding(x)\n",
    "        # print(x.shape)\n",
    "        output, hidden = self.lstm(x)\n",
    "        # print(output.shape)\n",
    "        attn_weights = self.attention(output)\n",
    "        # print(attn_weights.shape)\n",
    "        attn_output = output * attn_weights\n",
    "        # print(attn_output.shape)\n",
    "        attn_output = torch.sum(attn_output, dim=-1)\n",
    "        # print(attn_output.shape)\n",
    "        x = self.fc2(attn_output)\n",
    "        # print(x.shape)\n",
    "        x = self.softmax(x)\n",
    "        # print(x.shape)\n",
    "        x = x.squeeze()\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "dataset df len: 1600000\n",
      "vectorizer vocab len:  2369\n"
     ]
    }
   ],
   "source": [
    "# 현재 사용가능한 디바이스로 환경변수 device 재설정\n",
    "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(args.device)\n",
    "# 학습에 사용할 데이터셋 파일 가져와서 Dataset 객체 만들기 \n",
    "dataset = CstateDataset.load_dataset_and_make_vectorizer(dataset_df)\n",
    "# dataset 객체를 만들면 안에서 vectorizer 객체도 생성되기 때문에 여기서 vectorizer 뽑아낼 수 있음\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Prefetcher(args, len(vectorizer.pa_vocab)).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2369"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.pa_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "# 학습률을 매 스텝마다 0.5배로 조정\n",
    "# 아래 학습의 경우, 에폭마다 0.5배하도록 했음 -> 적은데이터 수로 인해 학습속도가 매우 빨라서 매 에폭마다 조절 필요하다고 판단함\n",
    "# 어차피 매 에폭마다 학습률 줄이는 방향으로 조절할 것이기 때문에 가장 단순한 StepLR 통해 학습률을 조정하고자 했음\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (X / X Steps) (loss=X.X): 0it [00:00, ?it/s]/tmp/ipykernel_90061/2672553311.py:30: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n",
      "Training (1 / 12500 Steps) (loss=7.77438): : 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (12499 / 12500 Steps) (loss=6.29466): : 12500it [12:27, 16.72it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=5.51433): : 5000it [02:39, 31.29it/s]\n",
      "Training (12499 / 12500 Steps) (loss=5.10047): : 12500it [12:24, 16.79it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=5.17477): : 5000it [02:40, 31.24it/s]\n",
      "Training (12499 / 12500 Steps) (loss=4.49891): : 12500it [12:19, 16.90it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=5.14660): : 5000it [02:40, 31.06it/s]\n",
      "Training (12499 / 12500 Steps) (loss=4.21748): : 12500it [12:21, 16.87it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=5.06553): : 5000it [02:40, 31.11it/s]\n",
      "Training (12499 / 12500 Steps) (loss=4.15906): : 12500it [12:22, 16.83it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=5.01620): : 5000it [02:40, 31.07it/s]\n",
      "Training (12499 / 12500 Steps) (loss=4.13451): : 12500it [12:27, 16.72it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=4.97807): : 5000it [02:41, 30.94it/s]\n",
      "Training (12499 / 12500 Steps) (loss=4.05425): : 12500it [12:24, 16.79it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=4.95241): : 5000it [02:41, 30.97it/s]\n",
      "Training (12499 / 12500 Steps) (loss=3.99749): : 12500it [12:22, 16.84it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=4.89825): : 5000it [02:41, 31.00it/s]\n",
      "Training (12499 / 12500 Steps) (loss=3.89251): : 12500it [12:28, 16.71it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=4.90794): : 5000it [02:41, 30.94it/s]\n",
      "Training (12499 / 12500 Steps) (loss=3.91005): : 12500it [12:28, 16.70it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=4.91844): : 5000it [02:41, 30.97it/s]\n",
      "Training (12499 / 12500 Steps) (loss=3.82101): : 12500it [12:25, 16.77it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=4.92408): : 5000it [02:41, 30.97it/s]\n",
      "Training (12499 / 12500 Steps) (loss=3.83963): : 12500it [12:29, 16.68it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=4.87238): : 5000it [02:41, 30.93it/s]\n",
      "Training (12499 / 12500 Steps) (loss=3.95994): : 12500it [12:27, 16.73it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=4.90644): : 5000it [02:41, 30.99it/s]\n",
      "Training (12499 / 12500 Steps) (loss=3.83644): : 12500it [12:29, 16.67it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=4.93443): : 5000it [02:41, 31.00it/s]\n",
      "Training (12499 / 12500 Steps) (loss=3.86117): : 12500it [12:29, 16.68it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=4.91531): : 5000it [02:41, 30.96it/s]\n",
      "Training (12499 / 12500 Steps) (loss=3.76989): : 12500it [12:26, 16.75it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=4.92729): : 5000it [02:41, 30.91it/s]\n",
      "Training (12499 / 12500 Steps) (loss=3.79989): : 12500it [12:23, 16.81it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=4.94304): : 5000it [02:41, 31.06it/s]\n",
      "Training (12499 / 12500 Steps) (loss=3.84631): : 12500it [12:27, 16.72it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=4.92129): : 5000it [02:41, 30.92it/s]\n",
      "Training (12499 / 12500 Steps) (loss=3.73429): : 12500it [12:30, 16.66it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=4.83597): : 5000it [02:41, 31.05it/s]\n",
      "Training (12499 / 12500 Steps) (loss=3.72617): : 12500it [12:21, 16.85it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=4.83735): : 5000it [02:41, 30.91it/s]\n",
      "Training (12499 / 12500 Steps) (loss=3.66224): : 12500it [12:22, 16.83it/s]\n",
      "Validation (4999 / 12500 Steps) (loss=4.91655): : 5000it [02:41, 30.99it/s]\n",
      "Training (11979 / 12500 Steps) (loss=4.75185): : 11979it [11:52, 16.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m     epoch_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     44\u001b[0m     train_iterator\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m Steps) (loss=\u001b[39m\u001b[38;5;132;01m%2.5f\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m(batch_idx, total, loss))\n\u001b[0;32m---> 45\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# epoch loss log\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# output_train_df = pd.DataFrame({'epoch':epoch+1, 'loss':epoch_train_loss})\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# train_log_df = pd.concat([train_log_df, output_train_df])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# validation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/prefetch/lib/python3.9/site-packages/torch/tensor.py:221\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Tensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function(relevant_args):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    215\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    216\u001b[0m         relevant_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         retain_graph\u001b[38;5;241m=\u001b[39mretain_graph,\n\u001b[1;32m    220\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph)\n\u001b[0;32m--> 221\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/prefetch/lib/python3.9/site-packages/torch/autograd/__init__.py:130\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 130\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 학습 중 학습데이터 및 검증데이터에 대한 손실값을 출력하기 위한 변수\n",
    "epoch_train_loss = 0.0\n",
    "epoch_val_loss = 0.0\n",
    "# 학습 중 손실값 혹은 metric 적용에 대한 결과값 등을 기록해두기 위한 리스트\n",
    "logs=[]\n",
    "# 학습을 시작할 epoch 지정\n",
    "start_epoch = 0\n",
    "weight_file = f'models/model_{start_epoch}.pth'\n",
    "# pretrained model load\n",
    "if start_epoch!=0:\n",
    "    pre_weights = torch.load(weight_file, map_location=args.device)\n",
    "    model.load_state_dict(pre_weights)\n",
    "    \n",
    "for epoch in range(start_epoch, args.num_epoch):\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_val_loss = 0.0\n",
    "    # train_log_df = pd.DataFrame()\n",
    "    dataset.set_split('train')\n",
    "    train_batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "    total = dataset.get_num_batches(args.batch_size)\n",
    "    train_iterator = tqdm(train_batch_generator, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
    "    # for batch_idx, batch_dict in enumerate(train_batch_generator):\n",
    "    for batch_idx, batch_dict in enumerate(train_iterator):\n",
    "        # if batch_idx >= 24990:\n",
    "        #     print(batch_idx, \":\", batch_dict['x_data'].shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_dict['x_data'].to(args.device))\n",
    "        # 확률 분포로 변환 (소프트맥스 함수 사용)\n",
    "        # probabilities = F.softmax(y_pred, dim=1)\n",
    "        # loss = criterion(y_pred, batch_dict['y_target'].float().to(args.device))\n",
    "        # 타겟을 정수형으로 변환\n",
    "        # y_target = batch_dict['y_target'].to(torch.long).to(args.device)\n",
    "\n",
    "        # Negative Log Likelihood Loss를 사용하여 손실 계산\n",
    "        # loss = F.nll_loss(torch.log(probabilities), y_target)\n",
    "        # print(y_pred.shape)\n",
    "        # print(torch.argmax(y_pred, dim=1))\n",
    "        # print(y_pred)\n",
    "        # print(batch_dict['y_target'].shape)\n",
    "        # print(batch_dict['y_target'])\n",
    "        loss = criterion(y_pred, batch_dict['y_target'].squeeze().to(args.device))\n",
    "        epoch_train_loss += loss.item()\n",
    "        train_iterator.set_description(\"Training (%d / %d Steps) (loss=%2.5f)\" %(batch_idx, total, loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # epoch loss log\n",
    "    # output_train_df = pd.DataFrame({'epoch':epoch+1, 'loss':epoch_train_loss})\n",
    "    # train_log_df = pd.concat([train_log_df, output_train_df])\n",
    "    # train_log_df.to_csv(\"train_log.csv\")\n",
    "    # scheduler.step()\n",
    "\n",
    "    # validation\n",
    "    if(epoch+1) % 1 == 0:\n",
    "        # val_log_df = pd.DataFrame()\n",
    "        dataset.set_split('val')\n",
    "        val_batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        val_iterator = tqdm(val_batch_generator, desc=\"Validation (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
    "        for batch_idx, batch_dict in enumerate(val_iterator):\n",
    "            with torch.no_grad():\n",
    "                output = model(batch_dict['x_data'].to(args.device))\n",
    "                loss = criterion(output, batch_dict['y_target'].squeeze().to(args.device))\n",
    "                epoch_val_loss += loss.item()\n",
    "                val_iterator.set_description(\"Validation (%d / %d Steps) (loss=%2.5f)\" %(batch_idx, total, loss))\n",
    "        # output_val_df = pd.DataFrame({'epoch':epoch+1, 'loss':epoch_val_loss})\n",
    "        # val_log_df = pd.concat([val_log_df, output_val_df])\n",
    "        # val_log_df.to_csv(\"val_log.csv\")\n",
    "    scheduler.step(epoch_val_loss/dataset.get_num_batches(32))\n",
    "    # scheduler.step()\n",
    "    log_epoch = {'epoch':epoch+1, 'train_loss':epoch_train_loss, 'val_loss':epoch_val_loss}\n",
    "    logs.append(log_epoch)\n",
    "    log_df = pd.DataFrame(logs)\n",
    "    log_df.to_csv(\"log_output.csv\")\n",
    "    \n",
    "    # model save\n",
    "    if(epoch+1) % 5 == 0:\n",
    "        torch.save(model.state_dict(), 'models/model_'+str(epoch+1)+'.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test (X / X Steps): 0it [00:00, ?it/s]/tmp/ipykernel_90061/2672553311.py:30: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n",
      "Test (0 / 480000 Steps): : 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test (479999 / 480000 Steps): : 480000it [2:35:27, 51.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203312\n",
      "42.35666666666666\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('models/model_20.pth'))\n",
    "model = model.to(args.device)\n",
    "result_df = pd.DataFrame()\n",
    "dataset.set_split('test')\n",
    "test_batch_generator = generate_batches(dataset, batch_size=1, device=args.device)\n",
    "test_iterator = tqdm(test_batch_generator, desc=\"Test (X / X Steps)\", dynamic_ncols=True)\n",
    "test_total = dataset.get_num_batches(1)\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    # for _, data in enumerate(test_batch_generator):\n",
    "    for idx, data in enumerate(test_iterator):\n",
    "        output = model(data['x_data'].to(args.device))\n",
    "        topk_vals, topk_indices = torch.topk(output, 20)\n",
    "        toplist = topk_indices.detach().cpu().tolist()\n",
    "        target = data['y_target'].squeeze().cpu().detach().numpy()\n",
    "        output_df = pd.DataFrame({'pred':[toplist], 'target':target})\n",
    "        result_df = pd.concat([result_df, output_df])\n",
    "        if target in toplist:\n",
    "            correct+=1\n",
    "        test_iterator.set_description(\"Test (%d / %d Steps)\" %(idx, test_total))\n",
    "print(correct)\n",
    "print(correct/test_total*100)\n",
    "result_df.to_csv('test_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.2905, -3.5925, -3.7852, -3.7986, -3.8506, -3.8742, -4.0041, -4.0068,\n",
       "        -4.0105, -4.0223])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_vals.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6231, 6180, 6182, 6444, 6103, 5933, 6216, 6226, 6262, 6081],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_10 = 0\n",
    "for idx, row in result_df.iterrows():\n",
    "    if row.target in row.pred[:10]:\n",
    "        cor_10+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.130000000000003"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_10 / len(result_df) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prefetch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
