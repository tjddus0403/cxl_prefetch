{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library & Module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    perc_train = 0.8,\n",
    "    perc_val = 0.2,\n",
    "    perc_test = 0,\n",
    "    perc_vocab = 1,\n",
    "    dataset = \"../simulator/deep_learning_data/bc_4m.cstate\",\n",
    "    dataset_csv = \"./csv_files/bc_4m.csv\",\n",
    "    vocabset = \"../simulator/deep_learning_data/bc_5m.cstate\",\n",
    "    vocabset_csv = \"./csv_files/bc_5m_all.csv\",\n",
    "    testset_csv = \"./csv_files/bc_test.csv\",\n",
    "    seed = 1337,\n",
    "    lr = 5e-4,\n",
    "    batch_size = 64,\n",
    "    num_epoch = 200,\n",
    "    embedding_size = 64,\n",
    "    encoding_size = 32,\n",
    "    cut_off = 1,\n",
    "    max_len = 128,\n",
    "    cuda = True,\n",
    "    device = 'cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# 재현성을 위해 시드 설정\n",
    "set_seed_everywhere(args.seed, args.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = list()\n",
    "output_list = list()\n",
    "with open(args.dataset, 'r') as dataset:\n",
    "    line = dataset.readline().split()\n",
    "    while line:\n",
    "        strline = [str(dstr) for dstr in line]\n",
    "        input_list.append(\" \".join(strline[:-1]))\n",
    "        output_list.append(strline[-1])\n",
    "        line = dataset.readline().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_input_list = list()\n",
    "voc_output_list = list()\n",
    "with open(args.vocabset, 'r') as vocabset:\n",
    "    line = vocabset.readline().split()\n",
    "    while line:\n",
    "        strline = [str(dstr) for dstr in line]\n",
    "        voc_input_list.append(\" \".join(strline[:-1]))\n",
    "        voc_output_list.append(strline[-1])\n",
    "        line = vocabset.readline().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.DataFrame(input_list, columns=['pa'])\n",
    "final_data['label'] = output_list\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_voc = pd.DataFrame(voc_input_list, columns=['pa'])\n",
    "final_voc['label'] = voc_output_list\n",
    "final_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test = final_voc.iloc[-1000000:-900000]\n",
    "final_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv(args.dataset_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_voc.to_csv(args.vocabset_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test.to_csv(args.testset_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(args.dataset_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabset_df = pd.read_csv(args.vocabset_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df = pd.read_csv(args.testset_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['label'] = dataset_df['label'].astype(str)\n",
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabset_df['label'] = vocabset_df['label'].astype(str)\n",
    "vocabset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df['label'] = testset_df['label'].astype(str)\n",
    "testset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datatset / DataLoader / Vectorizer / Vocabulary / Model의 흐름에 대한 구성\n",
    "순서 : Vocabulary -> Vectorizer -> Dataset -> DataLoader -> Model\n",
    "\n",
    "Raw Data\n",
    "- 현 시점 cache에 존재하는 주소 목록 \n",
    "- ex: 125, 158, 154, 134, 145, 341, 133, 136\n",
    "- 주소를 숫자가 아닌 문자열로 보아야 함 (physical addr는 숫자의 의미보다 문자열의 의미가 더 강하다고 판단)\n",
    "- 주소를 하나의 토큰으로\n",
    "\n",
    "Vocabulary \n",
    "- 각 주소(토큰)를 정수로 매핑\n",
    "- 입력 및 출력의 대상이 되는 주소만을 가짐 (+-10정도 커버?)\n",
    "\n",
    "** Vectorizer ** \n",
    "- 매핑된 토큰을 벡터 형태로 변환\n",
    "\n",
    "Dataset \n",
    "- vectorizer 이용하여 구성\n",
    "\n",
    "DataLoader\n",
    "- 미니배치 단위로 데이터셋 가져옴\n",
    "\n",
    "Model\n",
    "- 초기값: 첫 주소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAVocabulary(object):\n",
    "    def __init__(self, token_to_idx = None, add_unk = True,\n",
    "                 mask_token = \"<MASK>\", unk_token = \"<UNK>\"):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx:token for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(self._unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self._token_to_idx, \n",
    "                'add_unk': self._add_unk, \n",
    "                'unk_token': self._unk_token}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, cstates):\n",
    "        return cls(**cstates)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" %index)\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\"%len(self)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer\n",
    "- max_len에 대한 조건 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAVectorizer(object):\n",
    "    def __init__(self, pa_vocab):\n",
    "        self.pa_vocab = pa_vocab\n",
    "    \n",
    "    def _vectorize(self, indices):\n",
    "        vector_length = len(indices)\n",
    "        vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        vector[:len(indices)] = indices\n",
    "        return vector\n",
    "        \n",
    "    def _get_pa_indices(self, pa_list):\n",
    "        # 벡터로 변환된 physical addr list 반환\n",
    "        indices = [self.pa_vocab.lookup_token(token) for token in pa_list.split(\" \")]\n",
    "        return indices\n",
    "    \n",
    "    def vectorize(self, cstate):\n",
    "        pa_indices = self._get_pa_indices(cstate)\n",
    "        pa_vector = self._vectorize(indices=pa_indices)\n",
    "        return {'pa_vector':pa_vector,\n",
    "                'pa_length':len(pa_indices)}\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, cstate_df):\n",
    "        pa_vocab = PAVocabulary()\n",
    "        pa_counts = Counter()\n",
    "        for cstate in cstate_df.pa:\n",
    "            for pa in cstate.split(\" \"):\n",
    "                pa_counts[pa] += 1\n",
    "        for cstate in cstate_df.label:\n",
    "            pa_counts[cstate] += 1\n",
    "        \n",
    "        for pa, count in pa_counts.items():\n",
    "            if count >= args.cut_off:\n",
    "                pa_vocab.add_token(pa)\n",
    "        print(\"vectorizer vocab len: \",len(pa_vocab))\n",
    "        return cls(pa_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, cstate_dict):\n",
    "        pa_vocab = PAVocabulary.from_serializable(cstate_dict['pa_vocab'])\n",
    "        return cls(pa_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'pa_vocab': self.pa_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CstateDataset(Dataset):\n",
    "    def __init__(self, cstate_df, vectorizer, vocabset_df, testset_df):\n",
    "        self.cstate_df = cstate_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        self.n_total = len(cstate_df)\n",
    "        \n",
    "        # 3,000,000개 넣고 cstate 뽑았을 때의 데이터셋의 총 크기 넣어줌\n",
    "        self.train_size = 63784\n",
    "        self.train_df = self.cstate_df.loc[:self.train_size]\n",
    "        \n",
    "        # 4,000,000개 넣고 cstate 뽑았을 때의 데이터셋의 총 크기 - 3,000,000개 넣고 cstate 뽑았을 때의 데이터셋의 총 크기 \n",
    "        self.val_size = 85819 - 63784\n",
    "        self.val_df = self.cstate_df.loc[self.train_size : self.train_size+self.val_size]\n",
    "        \n",
    "        self.test_size = len(testset_df)\n",
    "        self.test_df = testset_df\n",
    "        \n",
    "        self.vocab_df_size = len(vocabset_df)\n",
    "        self.vocab_df = vocabset_df\n",
    "        \n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.val_size),\n",
    "                             'test': (self.test_df, self.test_size),\n",
    "                             'vocab': (self.vocab_df, self.vocab_df_size)}\n",
    "        \n",
    "        self.set_split('vocab')\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, cstate_df, vocabset_df, testset_df):\n",
    "        vocab_cstate_df = vocabset_df\n",
    "        print(\"dataset df len:\", len(vocab_cstate_df))\n",
    "        vectorizer = PAVectorizer.from_dataframe(vocab_cstate_df)\n",
    "        \n",
    "        return cls(cstate_df, vectorizer, vocabset_df, testset_df)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, cstate_df, vectorizer_filepath, vocabset_df, testset_df):\n",
    "        vocab_cstate_df = vocabset_df\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(cstate_df, vectorizer, vocab_cstate_df, testset_df)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return PAVectorizer.from_serializable(json.load(fp))\n",
    "        \n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "            \n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split='train'):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        pa_vector = self._vectorizer.vectorize(row.pa)\n",
    "        label_vector = self._vectorizer.vectorize(row.label)\n",
    "        \n",
    "        return {'x_data': pa_vector['pa_vector'],\n",
    "                'y_target': label_vector['pa_vector'],\n",
    "                'x_data_length': pa_vector['pa_length']}\n",
    "        \n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=False, drop_last=True, device='cpu'):\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict={}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn_fc = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.attn_fc(x)\n",
    "        # print(x.shape)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prefetcher(nn.Module):\n",
    "    def __init__(self, args, vocab_size):\n",
    "        super(Prefetcher, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                      embedding_dim=args.embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size = args.embedding_size,\n",
    "                            hidden_size=args.encoding_size,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True,\n",
    "                            dropout=0.1)\n",
    "        self.attention = Attention(args.encoding_size*2)\n",
    "        self.fc2 = nn.Linear(in_features=args.max_len, out_features=vocab_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.embedding(x)\n",
    "        # print(x.shape)\n",
    "        output, hidden = self.lstm(x)\n",
    "        # print(output.shape)\n",
    "        attn_weights = self.attention(output)\n",
    "        # print(attn_weights.shape)\n",
    "        attn_output = output * attn_weights\n",
    "        # print(attn_output.shape)\n",
    "        attn_output = torch.sum(attn_output, dim=-1)\n",
    "        # print(attn_output.shape)\n",
    "        x = self.fc2(attn_output)\n",
    "        # print(x.shape)\n",
    "        x = self.softmax(x)\n",
    "        # print(x.shape)\n",
    "        x = x.squeeze()\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 사용가능한 디바이스로 환경변수 device 재설정\n",
    "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(args.device)\n",
    "# 학습에 사용할 데이터셋 파일 가져와서 Dataset 객체 만들기 \n",
    "dataset = CstateDataset.load_dataset_and_make_vectorizer(dataset_df, vocabset_df, testset_df)\n",
    "# dataset 객체를 만들면 안에서 vectorizer 객체도 생성되기 때문에 여기서 vectorizer 뽑아낼 수 있음\n",
    "vectorizer = dataset.get_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Prefetcher(args, len(vectorizer.pa_vocab)).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.pa_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "# 학습률을 매 스텝마다 0.5배로 조정\n",
    "# 아래 학습의 경우, 에폭마다 0.5배하도록 했음 -> 적은데이터 수로 인해 학습속도가 매우 빨라서 매 에폭마다 조절 필요하다고 판단함\n",
    "# 어차피 매 에폭마다 학습률 줄이는 방향으로 조절할 것이기 때문에 가장 단순한 StepLR 통해 학습률을 조정하고자 했음\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.5, patience=1)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 중 학습데이터 및 검증데이터에 대한 손실값을 출력하기 위한 변수\n",
    "epoch_train_loss = 0.0\n",
    "epoch_val_loss = 0.0\n",
    "# 학습 중 손실값 혹은 metric 적용에 대한 결과값 등을 기록해두기 위한 리스트\n",
    "logs=[]\n",
    "# 학습을 시작할 epoch 지정\n",
    "start_epoch = 0\n",
    "weight_file = f'models/model_{start_epoch}.pth'\n",
    "# pretrained model load\n",
    "if start_epoch!=0:\n",
    "    pre_weights = torch.load(weight_file, map_location=args.device)\n",
    "    model.load_state_dict(pre_weights)\n",
    "    \n",
    "for epoch in range(start_epoch, args.num_epoch):\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_val_loss = 0.0\n",
    "    # train_log_df = pd.DataFrame()\n",
    "    dataset.set_split('train')\n",
    "    train_batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "    total = dataset.get_num_batches(args.batch_size)\n",
    "    train_iterator = tqdm(train_batch_generator, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
    "\n",
    "    for batch_idx, batch_dict in enumerate(train_iterator):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_dict['x_data'].to(args.device))\n",
    "        # 확률 분포로 변환 (소프트맥스 함수 사용)\n",
    "        # probabilities = F.softmax(y_pred, dim=1)\n",
    "        # loss = criterion(y_pred, batch_dict['y_target'].float().to(args.device))\n",
    "        # 타겟을 정수형으로 변환\n",
    "        # y_target = batch_dict['y_target'].to(torch.long).to(args.device)\n",
    "\n",
    "        # Negative Log Likelihood Loss를 사용하여 손실 계산\n",
    "        # loss = F.nll_loss(torch.log(probabilities), y_target)\n",
    "        # print(y_pred.shape)\n",
    "        # print(torch.argmax(y_pred, dim=1))\n",
    "        # print(y_pred)\n",
    "        # print(batch_dict['y_target'].shape)\n",
    "        # print(batch_dict['y_target'])\n",
    "        loss = criterion(y_pred, batch_dict['y_target'].squeeze().to(args.device))\n",
    "        epoch_train_loss += loss.item()\n",
    "        train_iterator.set_description(\"Training (%d / %d Steps) (loss=%2.5f)\" %(batch_idx, total, loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # validation\n",
    "    if(epoch+1) % 1 == 0:\n",
    "        dataset.set_split('val')\n",
    "        val_batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "        val_iterator = tqdm(val_batch_generator, desc=\"Validation (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
    "        for batch_idx, batch_dict in enumerate(val_iterator):\n",
    "            with torch.no_grad():\n",
    "                output = model(batch_dict['x_data'].to(args.device))\n",
    "                loss = criterion(output, batch_dict['y_target'].squeeze().to(args.device))\n",
    "                epoch_val_loss += loss.item()\n",
    "                val_iterator.set_description(\"Validation (%d / %d Steps) (loss=%2.5f)\" %(batch_idx, total, loss))\n",
    "\n",
    "    scheduler.step(epoch_val_loss/dataset.get_num_batches(32))\n",
    "    # scheduler.step()\n",
    "    log_epoch = {'epoch':epoch+1, 'train_loss':epoch_train_loss, 'val_loss':epoch_val_loss}\n",
    "    logs.append(log_epoch)\n",
    "    log_df = pd.DataFrame(logs)\n",
    "    log_df.to_csv(\"log_output.csv\")\n",
    "    \n",
    "    # model save\n",
    "    if(epoch+1) % 5 == 0:\n",
    "        torch.save(model.state_dict(), 'models/model_'+str(epoch+1)+'.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('models/model_15.pth'))\n",
    "model = model.to(args.device)\n",
    "result_df = pd.DataFrame()\n",
    "dataset.set_split('test')\n",
    "test_batch_generator = generate_batches(dataset, batch_size=1, device=args.device)\n",
    "test_iterator = tqdm(test_batch_generator, desc=\"Test (X / X Steps)\", dynamic_ncols=True)\n",
    "test_total = dataset.get_num_batches(1)\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for idx, data in enumerate(test_iterator):\n",
    "        output = model(data['x_data'].to(args.device))\n",
    "        topk_vals, topk_indices = torch.topk(output, 10)\n",
    "        toplist = topk_indices.detach().cpu().tolist()\n",
    "        target = data['y_target'].squeeze().cpu().detach().numpy()\n",
    "        output_df = pd.DataFrame({'pred':[toplist], 'target':target})\n",
    "        result_df = pd.concat([result_df, output_df])\n",
    "        if target in toplist:\n",
    "            correct+=1\n",
    "        test_iterator.set_description(\"Test (%d / %d Steps)\" %(idx, test_total))\n",
    "print(correct)\n",
    "print(correct/test_total*100)\n",
    "result_df.to_csv('./results/bc_test_result_idx.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idx to Addr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def idx_to_toklist(list_):\n",
    "    voc = vectorizer.pa_vocab\n",
    "    result=[]\n",
    "    for idx in list_:\n",
    "        result.append(voc.lookup_index(idx))\n",
    "    return result\n",
    "\n",
    "def str_to_list(strlist):\n",
    "    return literal_eval(strlist)\n",
    "\n",
    "def idx_to_tok(idx):\n",
    "    voc = vectorizer.pa_vocab\n",
    "    return voc.lookup_index(idx)\n",
    "\n",
    "result_idx = pd.read_csv(\"./results/bc_test_result_idx.csv\")\n",
    "result_idx['pred'] = result_idx['pred'].apply(str_to_list)\n",
    "result_idx['pred'] = result_idx['pred'].apply(idx_to_toklist)\n",
    "result_idx['target'] = result_idx['target'].apply(idx_to_tok)\n",
    "result_idx.to_csv(\"./results/bc_test_result_addr.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prefetch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
